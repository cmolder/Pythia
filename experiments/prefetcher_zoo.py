#!/usr/bin/env python3

"""
Set up and evaluate sweeps for the Prefetcher Zoo
experiments.

Authors: Quang Duong and Carson Molder
"""

import argparse
import os
import sys
import shutil
import itertools
from collections import defaultdict

import pandas as pd
import numpy as np
from scipy import stats
from tqdm import tqdm

from exp_utils import condor, config, evaluate, pc_trace

# Defaults (TODO: Move to yml or launch args)
default_eval_csv = './out/prefetcher_zoo/prefetcher_zoo.csv'
default_pc_trace_metric = 'num_useful'

help_str = {
'help': '''usage: {prog} command [<args>]

Available commands:
    condor            Set up Prefetcher Zoo sweep on Condor
    
    eval              Parse and compute metrics on sweep results
    
    online_pc_trace   Parse a per-PC statistics file and generate PC traces of the
                      best prefetchers for each PC on each SimPoint, for use in
                      online evaluation via multi_pc_trace
                      
    offline_pc_trace  Parse a per-PC statistics file and generate prefetch traces
                      using the best prefetchers for each PC on each SimPoint, for
                      use in offline evaluation via from_file
                      
    help              Display this help message. Command-specific help messages
                      can be displayed with `{prog} help command`
'''.format(prog=sys.argv[0]),

'condor': '''usage: {prog} condor <config-file> [-v / --verbose] [-d / --dry-run]

Description:
    {prog} condor <config-file>
        Sets up a Prefetching Zoo sweep for use on Condor. <config-file> is a path to a 
        .yml file with the config (example: experiments/exp_utils/zoo.yml)
        
Options:
    -v / --verbose
        If passed, prints extra details about the experiment setup.
        
    -d / --dry-run
        If passed, builds the experiment but writes nothing to <experiment-dir>.
'''.format(
    prog=sys.argv[0], 
),
    
'eval': '''usage: {prog} eval <results-dir> [--output-file <output-file>] [--norm-baseline <baseline>]

Description:
    {prog} eval <results-dir>
        Runs the evaluation procedure on the ChampSim result files found in <results-dir>
        (i.e. champsim_results/) and outputs a CSV at the specified output path.

Options:
    -o / --output-file <output-file>
        Specifies what file path to save the stats CSV data to. This defaults to
        `{default_eval_csv}`.
              
    --pc
        If provided, will compute per-PC prefetch stats on the LLC, using results
        in <results-dir>/pc_pref_stats/
        
    --dry-run
        If passed, builds the spreadsheet but writes nothing to <output-file>.

Note:
    To get stats comparing performance to a no-prefetcher baseline, it is necessary
    to have run the base ChampSim binary on the same execution trace.

    Without the base data, relative performance data comparing MPKI and IPC will
    not be available and the coverage statistic will only be approximate.
'''.format(
    prog=sys.argv[0], 
    default_eval_csv=default_eval_csv
),
'online_pc_trace': '''usage: {prog} online_pc_trace <pc-stats-file> <output-dir> [-m / --metric <metric>]

Description:
    {prog} online_pc_trace <pc-stats-file> <output-dir>
        Parses a PC stats file, and for each PC in each trace, determines the best
        prefetcher under <metric>. These online traces are saved to the output dir, 
        for use in the multi_pc_trace prefetcher.
        
        <pc-stats-file> is generated by running {prog} eval and passing the --pc flag.
        
Options:
    -m / --metric <metric>
        Specifies what metric to evaluate prefetchers by. Currently, the options are:
        {metric_options}

    --dry-run
        If passed, builds the traces but writes nothing.
'''.format(
    prog=sys.argv[0],
    metric_options=pc_trace.metrics
),
    
'offline_pc_trace': '''usage: {prog} offline_pc_trace <pc-stats-file> <output-dir> [-m / --metric <metric>] [-t / --num-threads <num-threads>] [--dry-run]

Description:
    {prog} pc_trace <pc-stats-file> <pref-trace-dir>
        Parses a PC stats file, and for each PC in each trace, determines the best
        prefetcher under <metric>. For each simpoint, it builds a new "combined" 
        trace in <pref-trace-dir> that uses the prefetches from the best-performing 
        prefetcher on each PC.
        
        <pc-stats-file> is generated by running {prog} eval and passing the --pc flag.
        
Options:
    -m / --metric <metric>
        Specifies what metric to evaluate prefetchers by. Currently, the options are:
        {metric_options}
        
    -t / --num-threads <num-threads>
        If passed (along with an argument to -d), will run <num-thread> threads to
        perform the offline trace processing for multiple benchmarks simultaneously.
        
        **CHECK YOUR SYSTEM'S MEMORY**, you may use >1 GB of memory per thread.
        
    --dry-run
        If passed, builds the traces but writes nothing.
'''.format(
    prog=sys.argv[0],
    metric_options=pc_trace.metrics
),
}


"""
Condor
"""
def condor_command():
    """Condor command
    """
    if len(sys.argv) < 3:
        print(help_str['condor'])
        exit(-1)
        
    parser = argparse.ArgumentParser(usage=argparse.SUPPRESS, add_help=False)
    parser.add_argument('config_file', type=str)
    parser.add_argument('-v', '--verbose', action='store_true')
    parser.add_argument('-d', '--dry-run', action='store_true')
    args = parser.parse_args(sys.argv[2:])
    cfg = config.read_config(args.config_file)
    
    print('Setting up Condor Prefetcher Zoo experiment:')
    print('    ChampSim:')
    print('        # sim inst        :', cfg.champsim.sim_instructions, 'million')
    print('        # warmup inst     :', cfg.champsim.warmup_instructions, 'million')
    print('        track pc stats?   :', cfg.champsim.track_pc_stats)
    print('        track addr stats? :', cfg.champsim.track_addr_stats)
    print('        track prefetches? :', cfg.champsim.track_pref)
    print('    Directories:')
    print('        ChampSim          :', cfg.paths.champsim_dir)
    print('        Experiment        :', cfg.paths.exp_dir)
    print('        Traces            :', cfg.paths.trace_dir)
    print('        Degree CSV file   :', cfg.paths.degree_csv if 'degree_csv' in cfg.paths else 'None')
    print('    L1D:')
    print('        Pref. candidates  :', ', '.join(cfg.l1d.pref_candidates))
    print('        Max hybrid        :', cfg.l1d.max_hybrid)
    print('    L2C:')
    print('        Pref. candidates  :', ', '.join(cfg.l2c.pref_candidates))
    print('        Max hybrid        :', cfg.l2c.max_hybrid)
    print('    LLC:')
    print('        Sets              :', cfg.llc.sets)
    print('        Pref. candidates  :', ', '.join(cfg.llc.pref_candidates))
    print('        Replacement       :', cfg.llc.repl)
    print('        Max hybrid        :', cfg.llc.max_hybrid)
    print()
    
    condor.build_sweep(cfg, dry_run=args.dry_run, verbose=args.verbose)



"""
Eval
"""
def eval_command():
    """Eval command
    """
    parser = argparse.ArgumentParser(usage=argparse.SUPPRESS, add_help=False)
    parser.add_argument('results_dir', type=str)
    parser.add_argument('-o', '--output-file', type=str, default=default_eval_csv)
    parser.add_argument('--pc', action='store_true')
    parser.add_argument('--dry-run', action='store_true')
    args = parser.parse_args(sys.argv[2:])
    
    # Add champsim_results/ to the path if it wasn't provided.
    results_dir = args.results_dir
    if not results_dir.endswith('champsim_results/'):
        results_dir = os.path.join(results_dir, 'champsim_results/')
    
    print('Generating cumulative run statistics...')
    evaluate.generate_csv(
        results_dir,
        args.output_file,
        dry_run=args.dry_run
    )
    
    if args.pc:
        print('Generating per-PC run statistics...')
        evaluate.generate_pc_csv(
            results_dir,
            args.output_file.replace('.csv', '_pc_llc.csv'),
            level='llc',
            dry_run=args.dry_run
        )
        
        
"""
Online PC Trace
"""
def online_pc_trace_command():
    """Online PC trace command
    """ 
    parser = argparse.ArgumentParser(usage=argparse.SUPPRESS, add_help=False)
    parser.add_argument('pc_stats_file', type=str)
    parser.add_argument('output_dir', type=str)
    parser.add_argument('-m', '--metric', type=str, default=default_pc_trace_metric)
    parser.add_argument('--dry-run', action='store_true')
    args = parser.parse_args(sys.argv[2:])
    
    # Build PC traces, for evaluation online (multi_pc_trace)
    pc_trace.build_online_pc_traces(
        args.pc_stats_file,
        args.output_dir,
        args.metric,
        level='llc',
        dry_run=args.dry_run
    )
    
        

"""
Offline PC Trace
"""
def offline_pc_trace_command():
    """Offline PC trace command
    """
    parser = argparse.ArgumentParser(usage=argparse.SUPPRESS, add_help=False)
    parser.add_argument('pc_stats_file', type=str)
    parser.add_argument('pref_traces_dir', type=str)
    parser.add_argument('-m', '--metric', type=str, default=default_pc_trace_metric)
    parser.add_argument('-t', '--num_threads', type=int, default=1)
    parser.add_argument('--dry-run', action='store_true')

    args = parser.parse_args(sys.argv[2:])
    
    # Best prefetch traces, for evaluation offline (from_file)
    pc_trace.build_offline_pc_traces(
        args.pref_traces_dir,
        args.pc_stats_file,
        args.metric,
        level='llc',
        num_threads=args.num_threads,
        dry_run=args.dry_run
    )

"""
Help
"""
def help_command():
    """Help command
    """
    # If one of the available help strings, print and exit successfully
    if len(sys.argv) > 2 and sys.argv[2] in help_str:
        print(help_str[sys.argv[2]])
        exit()
    # Otherwise, invalid subcommand, so print main help string and exit
    else:
        print(help_str['help'])
        exit(-1)



"""
Main
"""
commands = {
    'condor': condor_command,
    'online_pc_trace': online_pc_trace_command,
    'offline_pc_trace': offline_pc_trace_command,
    'eval': eval_command,
    'help': help_command,
}

def main():
    # If no subcommand specified or invalid subcommand, print main help string and exit
    if len(sys.argv) < 2 or sys.argv[1] not in commands:
        print(help_str['help'])
        exit(-1)

    # Run specified subcommand
    commands[sys.argv[1]]()

if __name__ == '__main__':
    main()
