champsim:
    warmup_instructions: 25
    sim_instructions: 100
    track_pc_pref: false
    track_addr_pref: false
    branch_pred: 'perceptron'
l1d:
    pref_candidates: []
    max_hybrid: 1
l2c:
    pref_candidates: ['scooby', 'scooby_double']
    max_hybrid: 1
llc:
    sets: 2048
    repl: 'ship'
    pref_candidates: []
    max_hybrid: 1
pythia:
    scooby_dyn_level_threshold:
        - -100.0
        #- -87.5
        - -75.0
        #- -62.5
        - -50.0
        #- -37.5
        - -25.0
        #- -22.5
        - -20.0
        #- -17.5
        - -15.0
        #- -12.5
        - -10.0
        #- -7.5
        - -5.0
        #- -2.5
        - 0.0
        #- 2.5
        - 5.0
        #- 7.5
        - 10.0
        #- 12.5
        - 15.0
        #- 17.5
        - 20.0
        #- 22.5
        - 25.0
        #- 37.5
        - 50.0
        #- 62.5
        - 75.0
        #- 87.5
        - 100.0
    scooby_alpha: 0.006508802942367162
    scooby_gamma: 0
    scooby_epsilon: 0.0018228444309622588
    scooby_policy: 'EGreedy'
    scooby_learning_type: 'SARSA'   
    scooby_separate_lowconf_pt: true # Whether to use separate EQs for high- and low-confidence prefetches (Scooby Double only, for now).
    scooby_pt_size: 256
    scooby_lowconf_pt_size: 256
paths:
    exp_dir: '/scratch/cluster/cmolder/pythia_level/exp_spec06mini_llc_aware_gamma_0_separate_eq/'
    trace_dir: '/scratch/cluster/cmolder/traces/spec06mini/champsim/'
    champsim_dir: '/u/cmolder/GitHub/Pythia/'
condor:
    user: 'cmolder@cs.utexas.edu'
    group: 'GRAD'
    project: 'ARCHITECTURE'
    description: 'Multi-level Pythia'